*********************************************************
*********************************************************
	Cancer prediction using Linear Regression 
*********************************************************
*********************************************************
—- List to make sure my dataset is properly uploaded from my local drive to the server

ls

-- copyFromLocal to HDFS

hadoop fs -copyFromLocal cancer.csv /BigData

— List again to see any changes

ls

— List BigData folder to check if cancer.csv is in it

hadoop fs -ls /BigData/.

-- Run spark-shell

spark-shell --master yarn
 

//-- Just a bunch of import statements, you can run these in a paste command

import org.apache.spark.sql.functions._
import org.apache.spark.ml.feature.{VectorAssembler}
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.regression.{LinearRegression}
import org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel, ParamGridBuilder}
import org.apache.spark.ml.evaluation.{RegressionEvaluator}
import org.apache.spark.ml.param.ParamMap
import org.apache.spark.sql.types.{IntegerType}

/*
-- Loading our CSV file, note the inferSchema option being set to true
*/

val data = spark.read
 .format("csv")
 .option("header", "true")
 .load("hdfs://10.128.0.2:8020/BigData/cancer.csv")


/*
-- Here we are selecting all the columns from our dataset to cast integer types
*/

val cancer = data.select(col("id").cast(IntegerType),col("Clump_Thickness").cast(IntegerType),col("UofCSize").cast(IntegerType),col("UofCShape").cast(IntegerType),col("Marginal_Adhesion").cast(IntegerType),col("SECSize").cast(IntegerType),col("Bare_Nuclei").cast(IntegerType),col("Bland_Chromatin").cast(IntegerType),col("Normal_Nucleoli").cast(IntegerType),col("Mitoses").cast(IntegerType),col("Class").cast(IntegerType))


/*
-- Next we split the dataset into training and test sets
-- We use a randomSplit function with a split percentage of 80%, and 20%
-- The second argument to the function is the seed, it is used to get the same random results every time
*/

val Array(trainingData, testData) = cancer.randomSplit(Array(0.8, 0.2), 1111) 


/*
### Machine Learning Steps ###

-- All the following steps will be the same for all machine learning problems

-- Preparing features and labels 

-- We need to prepare our features and labels to supply it to our algorithm, in this case linear regression
-- Remember features are the input data to our algorithm and labels are the values our algorithm is going to train the model to predict
-- Features are also called independent variables (Usually x-axis)
-- Lables are also called dependent variables (Usually y-axis)

-- In our case, FEATURES will be most of the columns except for Class
-- We will predict the Class, which becomes the LABEL
-- We will pass an array to the InputCols with the name of all the features 
-- And we will simply give the name of the output column

-- Think of a VectorAssembler as an Array
-- Vector assembler can be used to package one feature or multiple features into a vector
-- And this will be our features column
-- We pass an array to inputCols with the name of all the features we want to assemble
*/

val assembler = new VectorAssembler()
.setInputCols(Array("id","Clump_Thickness","UofCSize","UofCShape","Marginal_Adhesion","SECSize","Bare_Nuclei","Bland_Chromatin","Normal_Nucleoli","Mitoses"))
.setOutputCol("assembled-features")

/*
-- Next we will instantiate our algorithm, in this case linear regression, and set the features and label column
-- In this case, the features column will be the output from VectorAssembler which is the assembled features
-- And label is the value linear regression will try to predict which is the Class
-- Features column will be the output from the vector assembler
*/

val lr = new LinearRegression() 
 .setFeaturesCol("assembled-features")
 .setLabelCol("Class")

/*
-- Next we create a pipeline for everything that needs to be executed
-- Think of the pipeline as the assembly line in the factory where all the parts for a product are assembled together
-- This is what we are doing with pipeline
-- We create STAGES in pipelines
-- In this example, our pipeline has only two stages
-- The first stage is where we assemble the features with VectorAssembler
-- And the second stage, we'll specify the algorithm that is used to train the model
*/

val pipeline = new Pipeline()
 .setStages(Array(assembler, lr))

/*
-- Once we train the model, we need to evaluate the model for accuracy
-- Since this is a regression problem, we will use RegressionEvaluator to evaluate the model
-- The model, once it performs the predictions, it will store the prediction results in the predition column
-- Once a prediction is made, we need to evaluate the prediction with the actual value (Class), so we know how good the prediction is
-- So in our case, our model will predict the cancer (ie. 2 for no, 4 for yes)  on our score
-- We need to see if the the prediction matches the actual or not
-- If it doesn't match, we want to know how far the prediction is from the actual value of the Class
-- We will use a metric called r squared to show the accuracy of the model prediction
-- r squared is also called coefficient of determination
-- r squared basically tells us how good our model is
-- The higher the r2 model the better our model
*/

val evaluator = new RegressionEvaluator()
 .setLabelCol("Class")
 .setPredictionCol("prediction")
 .setMetricName("r2")

/*
-- Next we setup our cross validator
-- cross validator is an interesting concept
-- We need to provide two things to the CrossValidator
-- An estimator and an evaluator
-- Estimator we provide the pipeline
-- For the evaluator we provide regression evaluator 
-- We can provide additinoal parameters called "hyper parameters" which can be used for tuning our model
-- We don't have any hyper parameters so we create an instance of the parameter Grid Builder and call the build method
-- This will provide an empty parameter map
-- Finally we specify the fold as 3
-- This means the cross validator will generate three sets of data from our initial training dataset
-- In other words it "folds" the data into 3 and from each sets of data, it will use 2/3 of the data for training and 1/3 of the data for testing
-- And then it will fix the model based on the best accuracy based on the defined evaluation metric, in our case r2
*/

val cross_validator = new CrossValidator()
 .setEstimator(pipeline)
 .setEvaluator(evaluator)
 .setEstimatorParamMaps(new ParamGridBuilder().build)
 .setNumFolds(3)

/*
-- Next we call fit on the cross validator passing our trainig dataset
-- CrossValidator will now fold the dataset into 3, divide each subset into training and test set
-- Inside each fold, it will use the training set to train and test set to evaluate the model based on r2
-- The best of the three models is returned
*/

val cvModel = cross_validator.fit(trainingData)

/*
-- Now cvModel is a trained model that we can use to make predictions
-- We can actually save this model and reuse it as well
-- We'll see how to do that later

-- Before we can use our model on the real data, let's test our data on the real data to see how good it is
-- We now called cvModel.transform on the testData set
-- when we call transform, our model will go over the dataset and make predictions
*/

val predictions = cvModel.transform(testData)

/*
-- Remember, the directory SHOULD NOT EXIST, remove it first if it does
-- hadoop fs -rm -r /BigData/cancer/output/
-- Once the model has made predictions, we are just selecting the "id", "Class" and "prediction" column from the model
-- and write it as a CSV file in a location in HDFS
*/

predictions
 .select(col("id"), col("Class"), col("prediction"))
 .write
 .format("csv")
 .save("hdfs://10.128.0.2:8020/BigData/cancer/output/")

/*
-- Finally, we call the evaluator method on the evaluator passing the cancer DataFrame
-- It will give us back the r2 result
-- R2 is presented as a percentage, if we get 0.9, means our model is 90% accurate
*/

val r2 = evaluator.evaluate(predictions)

println("r-squared on test data = " + r2)

/* Prints: r-squared on test data = 0.8680452934382866 */